{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88965ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fdcbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"pdf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41011ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pdf/NASA UAM market Study 2018.pdf',\n",
       " 'pdf/Innovation Driving Sustainable Aviation - November 2021.pdf',\n",
       " 'pdf/roland_berger_urban_air_mobility_1.pdf',\n",
       " 'pdf/Roland_Berger_Urban_Air_Mobility 2018.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs = glob.glob(\"{}/*.pdf\".format(pdf_path))\n",
    "pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8686b4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhangquan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先要下载停用词，nltk自然语言处理包具有16种不同语言存储的停用词列表。\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fc3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "import hanlp\n",
    "# 因为是英文文档，所以直接使用hanlp的英文包\n",
    "#tokenizer = hanlp.utils.rules.tokenize_english \n",
    "#from hanlp.utils.lang.en.english_tokenizer import tokenize_english\n",
    "#tokenizer = tokenize_english\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "from collections import defaultdict\n",
    "import math\n",
    "import operator\n",
    "import pandas as pd\n",
    "import xlwt\n",
    "\n",
    "\n",
    "def feature_select(list_words):\n",
    "    #总词频统计\n",
    "    doc_frequency=defaultdict(int)\n",
    "    for word_list in list_words:\n",
    "        for i in word_list:\n",
    "            doc_frequency[i]+=1\n",
    " \n",
    "    #计算每个词的TF值\n",
    "    word_tf={}  #存储没个词的tf值\n",
    "    for i in doc_frequency:\n",
    "        word_tf[i]=doc_frequency[i]/sum(doc_frequency.values())\n",
    " \n",
    "    #计算每个词的IDF值\n",
    "    doc_num=len(list_words)\n",
    "    word_idf={} #存储每个词的idf值\n",
    "    word_doc=defaultdict(int) #存储包含该词的文档数\n",
    "    for i in doc_frequency:\n",
    "        for j in list_words:\n",
    "            if i in j:\n",
    "                word_doc[i]+=1\n",
    "    for i in doc_frequency:\n",
    "        word_idf[i]=math.log(doc_num/(word_doc[i]+1))\n",
    " \n",
    "    #计算每个词的TF*IDF的值\n",
    "    word_tf_idf={}\n",
    "    for i in doc_frequency:\n",
    "        word_tf_idf[i]=word_tf[i]*word_idf[i]\n",
    " \n",
    "    # 对字典按值由大到小排序\n",
    "    # 这里可以调整输出关键词的个数\n",
    "    dict_feature_select=sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return dict_feature_select[-10:]\n",
    "\n",
    "\n",
    "\n",
    "def extract_pdf_content(pdf):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    codec = 'utf-8'\n",
    "    outfp = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr=rsrcmgr, outfp=outfp, laparams=laparams)\n",
    "    with open(pdf, 'rb') as fp:\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        password = \"\"\n",
    "        maxpages = 0\n",
    "        caching = True\n",
    "        pagenos=set()\n",
    "        for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "            interpreter.process_page(page)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    #word_tokens = word_tokenize(outfp.getvalue()) \n",
    "    word_tokens = WhitespaceTokenizer().tokenize(outfp.getvalue()) \n",
    "    mystr = [w for w in word_tokens if not w in stop_words]\n",
    "    device.close()\n",
    "    outfp.close()\n",
    "    return mystr\n",
    "\n",
    "\n",
    "#  将数据写入新文件\n",
    "def data_write(file_path, datas, pdf):\n",
    "    f = xlwt.Workbook()\n",
    "    sheet1 = f.add_sheet(u'sheet1',cell_overwrite_ok=True) #创建sheet\n",
    "    \n",
    "    #将数据写入第 i 行，第 j 列\n",
    "    i = 0\n",
    "    for data in datas:\n",
    "        for j in range(len(data)):\n",
    "                sheet1.write(i,j,data[j])\n",
    "        i = i + 1        \n",
    "    f.save(file_path) #保存文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e636edfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content from pdf/NASA UAM market Study 2018.pdf ...\n",
      "Extracting content from pdf/Innovation Driving Sustainable Aviation - November 2021.pdf ...\n",
      "Extracting content from pdf/roland_berger_urban_air_mobility_1.pdf ...\n",
      "Extracting content from pdf/Roland_Berger_Urban_Air_Mobility 2018.pdf ...\n"
     ]
    }
   ],
   "source": [
    "mydict = {}\n",
    "datas = []\n",
    "j = 0\n",
    "for pdf in pdfs:    \n",
    "    key = pdf.split('/')[-1]    \n",
    "    if not key in mydict:        \n",
    "        print(\"Extracting content from {} ...\".format(pdf))  \n",
    "        mydict[key] = extract_pdf_content(pdf)\n",
    "        features=feature_select([mydict[key]])\n",
    "        #print(features[0])\n",
    "        data=[pdf,features[9][0],features[8][0],features[7][0],features[6][0],features[5][0],features[4][0],features[3][0],features[2][0],features[1][0],features[0][0]]\n",
    "        for i in range (0, len(data)):\n",
    "            data[i] = str (data[i])\n",
    "        str1 = \" \\n\"\n",
    "        str1 = str1.join(data)\n",
    "        with open(\"test.txt\",\"a\") as f:\n",
    "                f.write(str1)\n",
    "                f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b106a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e28740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e19250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
